[{"content":"突然换了一个新的博客发布方式。\n其实主要原因还是原来的博客服务器现在已经运行太多的服务从而不堪重负，apache时不时被OOM Kill导致博客也时不时跟着崩溃。\n加之原博客个人感觉更像一个笔记本，其记录的草稿一般的内容一直是我想改进的（虽然我的写作能力仍然不行）。\n所以，不如做一个fresh start，从一个新博客重新开始。\n新的博客是一个静态博客，采用hugo作为生成器。然而目前我对这个新玩意还很不熟悉，估计也需要很长一段时间才能适应吧～\n当然，原来的博客上仍然有不少有价值的文章，我会在后续的review过程中迁移过来，同时也希望能够改进文章质量，老博客已经迁移到了https://old-blog.kuludu.net/。\nAnyway, that is a new start.\n","date":"2024-06-15T19:45:00+08:00","permalink":"https://blog.kuludu.net/article/hello-new-blog/","title":"你好！新博客"},{"content":"目前我们的实验室有两条线路：\n172.C.D.0/24: 校园网（不可连接互联网，但可以接入VPN） 192.168.B.0/24: 实验室宽带网（可以连接互联网，但是没有公网IP） 我们的主要需求是通过校园网实现远程SSH接入，通过实验室宽带网访问互联网。但是由于Ubuntu生成的默认网关会自动将所有流量转到metric小的接口上，这将导致原本应该从校园网出口流出的SSH流量也走了实验室宽带网，从而无法建立SSH连接。这就需要对路由进行修改，可以通过netplan工具进行。\n由于校园网的路由已经建立且正确，所以在默认网络配置下的主要问题其实是主机内路由表能否正确找到到客户端的路由路径，而且由于VPN的存在，还需要考虑其产生的虚拟IP到主机的路由。\n编辑/etc/netplan/50-cloud-init.yaml（这是Ubuntu 24.04默认的网络配置文件，也有可能会变动），将路由项写入：\n1 2 3 4 5 6 7 8 9 10 11 12 network: ethernets: eno1: addresses: - 172.C.D.E/24 # 主机校园网IP routes: - to: 172.A.0.0/12 # 整个校园网的子网 via: 172.C.D.1 # 当前网关（下同） - to: 211.64.0.0/13 # VPN的虚拟IP子网 via: 172.C.D.1 eno2: dhcp4: true # 实验室宽带网（直接使用DHCP配置） 接下来使用netplan apply进行应用，就可以实现两条线路分工工作了。\n当然，实现上述目标还有其它方法（例如通过设置路由表使流量强制走来时的接口），但是这些做法稍显复杂，就不在此赘述了。\n","date":"2024-05-20T00:00:00Z","permalink":"https://blog.kuludu.net/article/%E5%A4%9A%E7%BD%91%E5%8F%A3%E7%8E%AF%E5%A2%83%E4%B8%8B%E9%85%8D%E7%BD%AEubuntu%E7%BD%91%E7%BB%9C%E8%B7%AF%E7%94%B1/","title":"多网口环境下配置Ubuntu网络路由"},{"content":"在Google Scholar或者其它数据库向EndNote20中导入引文的时候，有时候会发现其字段不受支持。如在默认设置中，Journal Article就没有Publisher这一个字段，虽然大多数论文引用格式并不要求写出出版商，但是对于某一些缺失的字段无法被添加还是会造成一些麻烦的。本文就文献导入到引用导出全流程介绍EndNote20自定义字段设置问题。\n在设置中修改Reference Types（引用类型） 首先，我们需要在Settings \u0026gt; Reference Types中修改默认的引用类型。在这里的修改是针对某一文章类型来的，这里根据需要可以修改两个常用类型Journal Article（期刊文章）和Conference Proceedings（会议出版物）（下同）。\n以Journal Article（期刊文章）为例，可以看到，Publisher字段其实是受支持的，不过在EndNote20中被默认隐藏了。将留空的Publisher字段重新填充上去即可启用该字段。\n如果有其它自定义字段，可以使用Custom 1 ~ 8八个栏位，将需要的字段名称填入即可。\n至此，我们可以在文献选项卡中看到新的字段了。\n修改默认的Import Filters（导入过滤器） 既然EndNote20默认关闭了Publisher字段，那么导入过滤器自然也是默认忽略该字段的了，在这一步我们需要重新启用它。\n在Tools \u0026gt; Import Filters中选择Open Filter Manager，可以看到一个名为EndNote Import过滤器，这便是需要修改的对象了。\n在这里可以直接点击Edit进行修改，不过还是更推荐将默认设置复制一份后再进行操作。在macOS下Filters的路径为EndNote20安装路径下的Filters文件夹，Windows的路径逻辑应该也相同。\n将%I标签对应的{IGNORE}的标记修改为Publisher即可，其它标签的含义可以搜索参考RIS文件的格式说明，自定义字段亦是如此。\n在导入文献的时候记得要选择新修改的过滤器。对于非RIS格式的引用，则需要修改对应的Import Filter。\n修改默认的Output Styles（导出格式） 由于在我的论文写作中几乎只使用LaTeX作为排版工具，在这里我着重介绍一下Bibliography的导出。\n在Tools \u0026gt; Output Styles中选择Open Style Manager，修改BibTex Export即可，同样地也推荐备份一遍默认设置。\n选择Bibliography选项卡，将需要的字段以同样的格式添加进去即可，注意修改对应的文章类型。\n同样地，若是需要修改纯文本的导出格式，可以修改Citations选项卡下的内容，读者可以搜索相关资料。\n至此，EndNote20已经可以完美地收录和导出所需的字段了。\n","date":"2023-08-21T14:41:30+08:00","permalink":"https://blog.kuludu.net/article/%E5%85%B3%E4%BA%8E%E6%96%87%E7%8C%AE%E5%AF%BC%E5%85%A5endnote20%E5%AD%97%E6%AE%B5%E9%BB%98%E8%AE%A4%E4%B8%8D%E6%94%AF%E6%8C%81%E7%9A%84%E9%97%AE%E9%A2%98/","title":"关于文献导入EndNote20字段默认不支持的问题"},{"content":"前一阵，后辈问了我一个问题：\n1 2 3 4 5 6 7 8 9 10 11 12 #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; int main() { vector\u0026lt;int\u0026gt; a = {1, 2, 3, 4, 5, 6}; auto p = lower_bound(a.begin(), a.end(), 3, greater\u0026lt;int\u0026gt;()); cout \u0026lt;\u0026lt; *p \u0026lt;\u0026lt; endl; return 0; } 为什么这段代码为什么执行结果为0？\n确实，从直观思考上来说，返回结果确实有些诡异。一时间我也不太理解。\n我的第一反应是比较 p与 a.end()的位置关系，发现 p=a.end()。也就是说，函数并没有在容器中找到比a大的元素，这并不符合预期的执行结果。\n遇事不决翻文档，于是我找到了这篇文章。其中详细介绍了 upper_bound函数的原型与定义，其中介绍中的这句话解决了这个问题：\n范围 [first, last) 必须已相对于表达式 !(value \u0026lt; element) 或 !comp(value, element) 划分，即所有令此表达式为 true 的元素必须前趋所有令此表达式为 false 的元素。完全排序的范围满足此判别标准。\n除了常提到的序列有序前提以外，lower_bound对重载的比较关系也有一定的要求，这也就导致了上例中 lower_bound函数不符合预期的执行。\n参考其给出了函数可能的实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 template\u0026lt;class ForwardIt, class T, class Compare\u0026gt; ForwardIt upper_bound(ForwardIt first, ForwardIt last, const T\u0026amp; value, Compare comp) { ForwardIt it; typename std::iterator_traits\u0026lt;ForwardIt\u0026gt;::difference_type count, step; count = std::distance(first,last); while (count \u0026gt; 0) { it = first; step = count / 2; std::advance(it, step); if (!comp(value, *it)) { first = ++it; count -= step + 1; } else count = step; } return first; } 也对应了介绍中的说法，当采用上例的函数模版重载大小关系，会使迭代器 it不断向后迭代，直至与 a.end()重合。\n所以，要使运行结果正确，可以对函数传递 a的反向迭代器 a.rbegin()与 a.rend()。即：\n1 2 3 4 5 6 7 8 9 10 11 12 #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; int main() { vector\u0026lt;int\u0026gt; a = {1, 2, 3, 4, 5, 6}; auto p = lower_bound(a.rbegin(), a.rend(), 3, greater\u0026lt;int\u0026gt;()); cout \u0026lt;\u0026lt; *p \u0026lt;\u0026lt; endl; return 0; } 或者使用 reverse函数先对序列进行反转，当然，这样会造成性能损失。\n","date":"2021-06-15T15:36:00+08:00","permalink":"https://blog.kuludu.net/article/lower_bound%E7%9A%84%E4%B8%80%E4%B8%AA%E6%9C%AA%E9%A2%84%E6%9C%9F%E7%9A%84%E6%89%A7%E8%A1%8C%E8%A1%8C%E4%B8%BA/","title":"lower_bound()的一个未预期的执行行为"},{"content":"OG（Histogram of Oriented Gridients）即方向梯度直方图，是一种图像特征提取的方法。其最早由法国研究员Dalal等人在CVPR-2005上提出，通过图像局部梯度方向的分布描述图像中的物体边缘。\n算法主要分为以下几个步骤：\n预处理 计算梯度 计算梯度方向直方图 重叠直方图归一化 获取HOG特征向量 预处理 在这一步中，可以对图像进行裁剪与缩放以及调整图像亮度，以便后续对图像处理。\n例如：\n幂次变换 对数变换 计算梯度 通过Sobel算子计算水平与竖直梯度，并计算合梯度的幅值与方向：\n$$ g=\\sqrt{g^2_{x}+g^2_{y}} $$\n$$ \\theta=\\arctan\\frac{g_y}{g_x} $$\n因为梯度方向取绝对值，所以$\\theta\\in[0, \\pi]$，方向相反的两个梯度会被认为是同一个。\n计算梯度方向直方图 将图像划分为$n\\times m$的cell，对每个cell计算方向梯度强度直方图。在这一步中，我们需要对梯度方向进行离散化，例如可以将数据分为9个深度为20的箱，从而形成一个长度为9向量。\n重叠直方图归一化 将$x\\times x$的cell划分为一个block，采用滑动窗口的策略，对block内每一个cell拼接而成的向量进行归一化操作。\n获取HOG特征向量 在上一步中，对于每一个block，我们都得到了一个长度为$bin_{depth}\\times x^2$的向量，共计$(n-1)\\times(m-1)$个。将这些向量拼接起来，就得到了我们需要求的HOG特征向量\n参考：\nhttps://zhuanlan.zhihu.com/p/85829145 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 import cv2 import os import numpy as np from skimage import io import matplotlib.pyplot as plt from skimage.feature import hog from sklearn.svm import SVC from sklearn.metrics import precision_score,recall_score TRAIN_COUNT = 500 TEST_COUNT = 100 def get_features(object_detect, count, test=False): if test: img_path = f\u0026#34;data/test_set/{object_detect}s/{object_detect}.%d.jpg\u0026#34; start = 4001 else: img_path = f\u0026#34;data/training_set/{object_detect}s/{object_detect}.%d.jpg\u0026#34; start = 1 if object_detect == \u0026#34;cat\u0026#34;: labels = np.array([0 for _ in range(count)]).reshape(-1, 1) else: labels = np.array([1 for _ in range(count)]).reshape(-1, 1) features = list() for i in range(start, start+count): print(img_path % i) # 读取图片 gray = cv2.imread(img_path % i, cv2.IMREAD_GRAYSCALE) # 尺寸缩放 gray = cv2.resize(gray, (128, 128)) # 中值滤波 gray = cv2.medianBlur(gray, 3) # HOG特征提取 hog_image = hog(gray, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(8, 8)) features.append(hog_image.flatten()) features = np.array(features) return features, labels def get_predict_img(img_path): gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) # 尺寸缩放 gray = cv2.resize(gray, (128, 128)) # 中值滤波 gray = cv2.medianBlur(gray, 3) normalised_blocks, hog_image = hog(gray, orientations=9, pixels_per_cell=( 8, 8), cells_per_block=(8, 8), visualise=True) return hog_image.reshape(1, -1) cat, cat_labels = get_features(object_detect=\u0026#34;cat\u0026#34;, count=TRAIN_COUNT) dog, dog_labels = get_features(object_detect=\u0026#34;dog\u0026#34;, count=TRAIN_COUNT) img = np.vstack([cat, dog]) labels = np.vstack([cat_labels, dog_labels]) res = np.hstack([img, labels]) clf = SVC(probability=True) data = res[:, :-1] labels = res[:, -1] clf.fit(data, labels) # ----------- 预测单张图片 --------------------------------- # test_img = get_predict_img(\u0026#34;training_set/cats/cat.38.jpg\u0026#34;) # pred = clf.predict(test_img) # print(pred) # ----------- 预测单张图片 --------------------------------- test_cat, test_cat_labels = get_features(object_detect=\u0026#34;cat\u0026#34;, count=TEST_COUNT, test=True) test_dog, test_dog_labels = get_features(object_detect=\u0026#34;dog\u0026#34;, count=TEST_COUNT, test=True) test_img = np.vstack([test_cat, test_dog]) test_labels = np.vstack([test_cat_labels, test_dog_labels]) pred = clf.predict(test_img) precision = precision_score(pred,test_labels) recall = recall_score(pred,test_labels) print(\u0026#34;实际类别:\u0026#34;,test_labels.flatten()) print(\u0026#34;预测类别:\u0026#34;,pred.flatten()) print(f\u0026#34;精准率:{precision}, 召回率:{recall}\u0026#34;) ","date":"2021-04-20T17:57:00+08:00","permalink":"https://blog.kuludu.net/article/hog/","title":"HOG"}]